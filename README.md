# ğŸ”’ Deep Learning Project 3 â€“ Jailbreaking Deep Models

This project explores adversarial robustness in deep image classifiers by implementing and evaluating adversarial attacks on a pretrained ResNet-34 model using ImageNet-1K subset data.

## ğŸ“‚ Contents

- `main.ipynb`: Complete implementation of all 5 tasks
- `DeepLearningProject3_Report.pdf`: Final AAAI-style report (with tables, plots, and visuals)
- `TestDataSet.zip`: Input test images (100-class ImageNet subset)
- `DeepLearningProject3_Report.pdf`: Final project writeup
- `.DS_Store`: Auto-generated system file (can be ignored)

---

## ğŸ§ª Tasks Overview

| Task | Description |
|------|-------------|
| **Task 1** | Baseline accuracy evaluation on clean ImageNet images |
| **Task 2** | FGSM attack on full image with $\ell_\infty$ constraint |
| **Task 3** | Stronger iterative attacks: I-FGSM and PGD |
| **Task 4** | Patch-constrained attacks (32Ã—32 region only) |
| **Task 5** | Transferability analysis to DenseNet-121 |

---

## ğŸ“Š Key Results

- **Baseline Accuracy (ResNet-34)**:  
  - Top-1: 76.00%  
  - Top-5: 94.20%

- **Best Attack (PGD)**:  
  - Top-1 Accuracy: **0.20%**  
  - Top-5 Accuracy: **14.20%**

- **Transfer to DenseNet-121 (PGD)**:  
  - Top-1 drops to **65.00%**  
  - Patch PGD drops to **59.80%**

- ğŸ” Results include full plots comparing:
  - Attack strength (FGSM, PGD, Patch)
  - Transferability across architectures

---

## ğŸ“· Visualizations

- ğŸ” Adversarial examples (FGSM, PGD, Patch)
- ğŸ“‰ Accuracy drop bar charts
- ğŸ” Transferability line plots

---

## ğŸ’¡ Lessons Learned

- Even imperceptible pixel-level changes can collapse model predictions.
- Iterative and patch attacks outperform vanilla FGSM.
- Transferability makes black-box attacks viable.

---

## ğŸ‘©â€ğŸ’» Authors

- **Anushka Garg**
- **Nishant Sharma**
- **Rahul Mallidi** 
